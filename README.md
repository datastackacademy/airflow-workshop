Apache Airflow Workshop - Part1
=================================

Apache Airflow is an open source tool for programmatically authoring, scheduling, and monitoring data pipelines. It has over 9 million downloads per month and an active OSS community. Airflow allows data practitioners to define their data pipelines as Python code in a highly extensible and infinitely scalable way.

<br>

In this workshop we will learn the basics of Apache Airflow by:

1. Running Airflow locally via [Astronomer CLI](https://docs.astronomer.io/astro/cli/overview)
2. Getting familiar with using **Airflow UI**
3. Understanding Airflow concepts such as **DAGs** and **Task Operators**
4. Authoring our own DAGs or _data pipelines_


## Prerequisites

This workshop requires the following components to be pre-installed on your machine:

- [Python](https://python.land/installing-python) version 3.7 or higher
- [VS Code](https://code.visualstudio.com/download) or your IDE of choice
- [Docker](https://docs.docker.com/get-docker/)

**Docker** is required to run Airflow containers locally.


Clone the Project
=================

First, let's clone the workshop from our git repo. You can find our repo at: []()

Open a command terminal and run:

```bash
git clone 
```

